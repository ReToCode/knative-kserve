apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: torchscript-grpc
  namespace: kserve-demo
  annotations:
    serving.knative.openshift.io/enablePassthrough: "true"
spec:
  predictor:
    triton:
      storageUri: gs://kfserving-examples/models/torchscript
      runtimeVersion: 20.10-py3
      ports:
        - containerPort: 9000
          name: h2c
          protocol: TCP
      env:
        - name: OMP_NUM_THREADS
          value: "1"
